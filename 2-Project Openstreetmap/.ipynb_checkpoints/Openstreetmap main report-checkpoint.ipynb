{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Wrangle OpenStreetMap Data\n",
    "Data Wrangling with MongoDB\n",
    "Jocelyn Moreau\n",
    "\n",
    "Map Area: Finistere sud, Bretagne, France\n",
    "https://export.hotosm.org/en/v3/exports/382e36c5-7823-4848-b664-6fcdc6b533ee\n",
    "\n",
    "\n",
    "The OSM is 50,1Mo.\n",
    "\n",
    "In this project, we are going to address the following points:\n",
    "1. Prepare and sample the OSM file\n",
    " * Number of tags (mapparser.py)\n",
    " * Analyse the problematic tags (tags.py)\n",
    " * Audit the file (audit.py)\n",
    "2. Import in MongoDB and requested analysis\n",
    "3. Additional Statistics and Ideas\n",
    " * Contributor statistics\n",
    " * Additional data exploration using MongoDB\n",
    "4. Conclusion\n",
    "\n",
    "## 1. Prepare and sample the OSM file\n",
    "For this project, I decided to work on a region of France, called Finistere sud, in Bretagne, France, a nice area for vacation!\n",
    "The full map is available as france-finistere-sud_export.osm (Size of 50,1Mo, so compliant with the >50Mo size) in the current Github repository. I extracted a sample using the sample_file.py script (see cell below).\n",
    "The result is saved in france-finistere-sud_export_sample.osm. This is the file I will use for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample done\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# code stored in sample_file.py\n",
    "\n",
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "\n",
    "OSM_FILE = \"france-finistere-sud_export.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"france-finistere-sud_export_sample.osm\"\n",
    "\n",
    "k = 10 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'w') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(str(ET.tostring(element, encoding='utf-8')))\n",
    "\n",
    "    output.write('</osm>')\n",
    "    print(\"sample done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Number of tags (mapparser.py)\n",
    "Counting the number of nodes for each file (using mapparser.py script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'member': 49,\n",
      " 'meta': 1,\n",
      " 'nd': 275503,\n",
      " 'node': 216346,\n",
      " 'note': 1,\n",
      " 'osm': 1,\n",
      " 'relation': 24,\n",
      " 'tag': 74529,\n",
      " 'way': 34610}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Map parser to count the number of tags of the OSMFILE\n",
    "mapparser.py\n",
    "\"\"\"\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if elem.tag in tags:\n",
    "            tags[elem.tag] += 1\n",
    "        else:\n",
    "            tags[elem.tag] = 1\n",
    "    return tags\n",
    "\n",
    "pprint.pprint(count_tags(OSM_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'member': 4,\n",
      " 'nd': 27548,\n",
      " 'node': 21635,\n",
      " 'osm': 1,\n",
      " 'relation': 2,\n",
      " 'tag': 7450,\n",
      " 'way': 3461}\n"
     ]
    }
   ],
   "source": [
    "# Count the tags for the sample file\n",
    "pprint.pprint(count_tags(SAMPLE_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 74018, 'lower_colon': 463, 'other': 42, 'problemchars': 6}\n"
     ]
    }
   ],
   "source": [
    "#script used 2-tags.py\n",
    "import re\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        for tag in element.iter('tag'):\n",
    "            k = tag.get('k')\n",
    "            if lower.search(k):\n",
    "                keys['lower'] += 1\n",
    "            elif lower_colon.search(k):\n",
    "                keys['lower_colon'] += 1\n",
    "            elif problemchars.search(k):\n",
    "                keys['problemchars'] += 1\n",
    "            else:\n",
    "                keys['other'] += 1\n",
    "        \n",
    "    return keys\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    \n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "keys = process_map(OSM_FILE)\n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legend:\n",
    "  * \"lower\", for tags that contain only lowercase letters and are valid,\n",
    "  * \"lower_colon\", for otherwise valid tags with a colon in their names,\n",
    "  * \"problemchars\", for tags with problematic characters, and\n",
    "  * \"other\", for other tags that do not fall into the other three categories.\n",
    "\n",
    "This first analysis shows us that there is:\n",
    " * 74018 tags having no upper case, which means that will have to add an uppercase for the first letter of the tag in the next step.\n",
    " * 463 tags with a colon (a short look shows these are web addresses)\n",
    " * 6 problematic characters in the map chosen\n",
    " * 42 others, which seem to be fine (they are tags for mentioning the source of the information, SIREN (French unique reference number for company) ref. ...\n",
    " \n",
    " ### 1.3 Audit the file (audit.py)\n",
    "After auditing the sample file with the audit.py script, I could not detect any error... so I decided to run the audit on the complete OSM file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get the list of existing name\n",
      "##############################\n",
      "{'Argant': {'Stang Argant'},\n",
      " 'Berthou': {'Rue Joseph Berthou'},\n",
      " 'Bienvenue': {'Rue Fulgence Bienvenue'},\n",
      " 'Bras': {'Hent Mort Bras'},\n",
      " 'Carnot': {'Quai Carnot'},\n",
      " 'Colguen': {'Rue de Colguen'},\n",
      " 'Courcy': {'rue de Courcy', 'Rue de Courcy', 'Rue Courcy'},\n",
      " 'Croix': {'Quai de la Croix'},\n",
      " 'Césaire': {'Rue Emile Césaire', 'Rue Aimé Césaire'},\n",
      " 'Duguesclin': {'Place Duguesclin'},\n",
      " 'Duquesne': {'Rue Duquesne'},\n",
      " 'Flaubert': {'Rue Gustave Flaubert'},\n",
      " 'Foch': {'Rue Maréchal Foch'},\n",
      " 'Gaulle': {'Place Général de Gaulle'},\n",
      " 'Guillou': {'Boulevard Alfred Guillou'},\n",
      " 'Guéguin': {'Avenue Pierre Guéguin'},\n",
      " 'Kerdavid': {'Kerdavid', 'Route de Kerdavid'},\n",
      " 'Lanriec': {'Rue de Lanriec'},\n",
      " 'Madec': {'Rue René Madec'},\n",
      " 'Malakoff': {'Malakoff'},\n",
      " 'Morvan': {'Rue du Général Morvan'},\n",
      " 'Neuve': {'Rue Neuve'},\n",
      " 'Novembre': {'Rue du 11 Novembre'},\n",
      " 'Pins': {'Allée des Pins'},\n",
      " 'Ruat-Vraz': {'Impasse de Ruat-Vraz'},\n",
      " 'Sables-Blancs': {'Rue des Sables-Blancs'},\n",
      " 'Schoelcher': {'Rue Victor Schoelcher'},\n",
      " 'Vert': {'Rue du Poteau Vert'},\n",
      " 'Vidie': {'Rue Lucien Vidie'},\n",
      " 'gare': {'rue de la gare'},\n",
      " \"l'Alma\": {\"Rue de l'Alma\"},\n",
      " \"l'Océan\": {\"Avenue de l'Océan\"},\n",
      " \"l'Église\": {\"Rue de l'Église\", \"Place de l'Église\"},\n",
      " 'neuve': {'rue neuve'},\n",
      " 'Écoles': {'Rue des Écoles'}}\n",
      "\n",
      "List of items with proposed correction:\n",
      "Rue des Écoles => Rue Des Écoles\n",
      "Avenue Pierre Guéguin => Avenue Pierre Guéguin\n",
      "Hent Mort Bras => Hent Mort Bras\n",
      "Impasse de Ruat-Vraz => Impasse De Ruat-Vraz\n",
      "Allée des Pins => Allée Des Pins\n",
      "Kerdavid => Kerdavid\n",
      "Route de Kerdavid => Route De Kerdavid\n",
      "Avenue de l'Océan => Avenue De L'Océan\n",
      "rue neuve => Rue Neuve\n",
      "Rue de l'Église => Rue De L'Église\n",
      "Place de l'Église => Place De L'Église\n",
      "Rue Fulgence Bienvenue => Rue Fulgence Bienvenue\n",
      "Rue du 11 Novembre => Rue Du 11 Novembre\n",
      "Rue des Sables-Blancs => Rue Des Sables-Blancs\n",
      "Boulevard Alfred Guillou => Boulevard Alfred Guillou\n",
      "Quai de la Croix => Quai De La Croix\n",
      "Rue Duquesne => Rue Duquesne\n",
      "rue de Courcy => Rue De Courcy\n",
      "Rue de Courcy => Rue De Courcy\n",
      "Rue Courcy => Rue Courcy\n",
      "Rue Maréchal Foch => Rue Maréchal Foch\n",
      "Malakoff => Malakoff\n",
      "Rue de l'Alma => Rue De L'Alma\n",
      "Rue du Général Morvan => Rue Du Général Morvan\n",
      "Place Général de Gaulle => Place Général De Gaulle\n",
      "Place Duguesclin => Place Duguesclin\n",
      "Rue Joseph Berthou => Rue Joseph Berthou\n",
      "Quai Carnot => Quai Carnot\n",
      "Rue de Lanriec => Rue De Lanriec\n",
      "rue de la gare => Rue De La Gare\n",
      "Rue Gustave Flaubert => Rue Gustave Flaubert\n",
      "Rue du Poteau Vert => Rue Du Poteau Vert\n",
      "Rue Neuve => Rue Neuve\n",
      "Stang Argant => Stang Argant\n",
      "Rue de Colguen => Rue De Colguen\n",
      "Rue Lucien Vidie => Rue Lucien Vidie\n",
      "Rue René Madec => Rue René Madec\n",
      "Rue Emile Césaire => Rue Emile Césaire\n",
      "Rue Aimé Césaire => Rue Aimé Césaire\n",
      "Rue Victor Schoelcher => Rue Victor Schoelcher\n"
     ]
    }
   ],
   "source": [
    "#audit.py\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "expected = [\"Concarneau\", \"Rue\", \"Impasse\", \"Avenue\", \"Pont\", \"Boulevard\", \"Porte\", \"Route\", \"Chemin\"] #expected names in the dataset\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "#Array to update with the errors encountered in the file\n",
    "mapping = {\"bretagne\": \"Bretagne\",\n",
    "           \"Av.\": \"Avenue\",\n",
    "           \"pont\": \"Pont\",\n",
    "           \"rue\": \"Rue\",\n",
    "           \"Bd\" : \"Boulevard\",\n",
    "           \"bd\" : \"Boulevard\",\n",
    "           \"Pt\" : \"Pont\",\n",
    "           \"Rte\": \"Route\",\n",
    "           \"place\": \"Place\",\n",
    "           \"quai\": \"Quai\",\n",
    "           \"chemin\": \"Chemin\"\n",
    "           }\n",
    "\n",
    "# Search string for the special characters and compare the result to the expected list. If not inside, then add it to the street_type array.\n",
    "def audit_street(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "# Check streetname\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "# Audit function\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, encoding=\"utf8\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street(street_types, tag.attrib['v'])\n",
    "\n",
    "    return street_types\n",
    "\n",
    " # reformat string to first letter capital, except for abreviation all in capital\n",
    "def string_case(s):\n",
    "    if s.isupper():\n",
    "        return s\n",
    "    else:\n",
    "        return s.title()\n",
    "\n",
    "# update name function\n",
    "def update_name(name, mapping):\n",
    "    name = name.split(' ')\n",
    "    for i in range(len(name)):\n",
    "        if name[i] in mapping:\n",
    "            name[i] = mapping[name[i]]\n",
    "            #reformat\n",
    "            name[i] = string_case(name[i])\n",
    "        else:\n",
    "            name[i] = string_case(name[i])\n",
    "\n",
    "    name = ' '.join(name)\n",
    "\n",
    "\n",
    "    return name\n",
    "print(\"Get the list of existing name\")\n",
    "print(\"##############################\")\n",
    "pprint.pprint(dict(audit(OSM_FILE))) # print the existing names\n",
    "\n",
    "print()\n",
    "print(\"List of items with proposed correction:\")\n",
    "\n",
    "st_types = audit(OSM_FILE)\n",
    "\n",
    "# print the updated names\n",
    "for street_type, ways in st_types.items():\n",
    "    for name in ways:\n",
    "        better_name = update_name(name, mapping)\n",
    "        print (name, \"=>\", better_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has a very good quality, very few mistake were detected.\n",
    "\n",
    "Lower case mispelling:\n",
    "* rue -> Rue\n",
    "\n",
    "I updated the audit.py dictionnary to include the detected mistake and ran the script again to correct all these mispelling.\n",
    "\n",
    "Abbreviations:\n",
    "There was no abbreviation in the map chosen (either a student from Udacity applied this training to correct the mistake, or Openstreetmap implemented a script to correct it) --> nothing to do here.\n",
    "\n",
    "## 2. Import in MongoDB\n",
    "### 2.1 Transform the xml into json format ()\n",
    "I used the following data_json.py script that turns the xml into json, cleaning the mistakes detected using the previously used functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# JSON file for import into Mongo DB\n",
      "--> Done!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Export to JSON for MONGO DB------------------------------------------------------------------------------\n",
    "# the corrected data are shaped in a dictionary in order to be saved into a JSON file in order to be imported by MongoDB later on.\n",
    "# The following operations are performed:\n",
    "# •only 2 types of top level tags: \"node\" and \"way\" are processed\n",
    "# •all attributes of \"node\" and \"way\" are turned into regular key/value pairs\n",
    "# •some attributes \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\" are added under a key \"created\"\n",
    "# •attributes for latitude and longitude are added to a \"pos\" array,\n",
    "# •address related items are added to the tag \"address\"\n",
    "# •other second level tag \"k\" are added to the field \"others\"\n",
    "# script 4-data.py\n",
    "import codecs\n",
    "import json\n",
    "import xml.etree.cElementTree as ET\n",
    "from bson import json_util\n",
    "\n",
    "# List of structure fields for created for the json file\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "#OSM_FILE = \"finistere-sud-france_export.osm\"\n",
    "\n",
    "def shape_element(element):\n",
    "# Create a JSON file as a preparation step for MongoDB DataBase\n",
    "# Shape the XML file into a JSON alike structure\n",
    "    node = {}\n",
    "    # process only 2 types of top level tags: \"node\" and \"way\"\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        # \"type\"\n",
    "        node['type'] = element.tag\n",
    "        # \"id\"\n",
    "        if 'id' in element.attrib:\n",
    "            node['id'] = element.get('id')\n",
    "        # \"visible\"\n",
    "        if 'visible' in element.attrib:\n",
    "            node['visible'] = \"true\"\n",
    "            node['visible'] = element.get('visible')\n",
    "        # \"pos\"\n",
    "        if 'lat' in element.attrib and 'lon' in element.attrib:\n",
    "            node['pos'] = [0,0]\n",
    "            node['pos'][0] = float(element.get('lat'))\n",
    "            node['pos'][1] = float(element.get('lon'))\n",
    "        # \"created\"\n",
    "        for key in element.attrib:\n",
    "            value = element.attrib[key]\n",
    "            if \"created\" not in node.keys():\n",
    "                    node[\"created\"] = {}\n",
    "            if key in CREATED:\n",
    "                node[\"created\"][key] = value\n",
    "        # 2nd level \"address\"\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            key = tag.attrib['k']\n",
    "            value = tag.attrib['v']\n",
    "            # Apply correction\n",
    "            if is_street_name(tag): # if the key value is \"addr:street\"\n",
    "                m = street_type_re.search(value)\n",
    "                if m:\n",
    "                    street_type = m.group() # Boulevard\n",
    "                    if street_type not in mapping:\n",
    "                        value = update_name(value, mapping)\n",
    "        # 2nd level \"others\"\n",
    "            elif not problemchars.match(key):\n",
    "                if \"others\" not in node.keys():\n",
    "                    node[\"others\"] = {}\n",
    "                node[\"others\"][key] = value\n",
    "        # 2nd level \"way\"\n",
    "        for tag in element.iter(\"nd\"):\n",
    "            if \"node_refs\" not in node.keys():\n",
    "                node[\"node_refs\"] = []\n",
    "            node[\"node_refs\"].append(tag.attrib['ref'])\n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_map(file_in, pretty = False):\n",
    "    file_out = \"{0}.json\".format(file_in)    \n",
    "    with open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2, default=json_util.default)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el, default=json_util.default) + \"\\n\")\n",
    "\n",
    "print (\"# JSON file for import into Mongo DB\")\n",
    "process_map(OSM_FILE, True)\n",
    "print ('--> Done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Import the generated json into MongoDB\n",
    "\n",
    "I imported the json file (70,1 Mo) size into a MongoDB database (called francefinisteresud), using the mongoimport library. As a GUI for the database requests, I will use MongoBooster and copy paste below the result of the requests.\n",
    "\n",
    "## 2.3 Data overview\n",
    "Let's check the data we imported:\n",
    "\n",
    "#### Number of documents\n",
    "\n",
    "> db.francefinisteresud.find({}).count()\n",
    "\n",
    "-> 250950\n",
    "\n",
    "#### Number of unique users\n",
    "\n",
    "> db.francefinisteresud.distinct('created.user').length\n",
    "\n",
    "-> 90 differents users\n",
    "\n",
    "\n",
    "#### Number of nodes\n",
    "\n",
    "> db.francefinisteresud.find({\"type\":\"node\"}).count()\n",
    "\n",
    "-> 216346 results (which corresponds to the number of nodes we counted in the xml file)\n",
    "\n",
    "#### Number of ways\n",
    "\n",
    "> db.francefinisteresud.find({\"type\":\"way\"}).count()\n",
    "\n",
    "-> 34604 (instead of 34610: 6 ways were not imported, error message was as following: key ref:clochers.org must not contain '.', I will not try to fix this error for now)\n",
    "\n",
    "#### Number of schools:\n",
    "\n",
    "> db.francefinisteresud.find({'others.amenity': \"school\"}).count()\n",
    "--> 10\n",
    "\n",
    "#### Number of restaurants\n",
    "> db.francefinisteresud.find({'others.amenity': \"restaurant\"}).count()\n",
    "--> 21\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Additional Statistics and Ideas\n",
    "#### Top 10 users\n",
    "> db.francefinisteresud.aggregate([{'$group': {'_id': '$created.user', 'count': {'$sum' : 1}}}, {'$sort': {'count' : -1}}, {'$limit': 10}])\n",
    "\n",
    "The top ten users is as follow:\n",
    "![top 10 users result](./top10users.png)\n",
    "\n",
    "#### count of each different amenity referenced:\n",
    "> db.francefinisteresud.aggregate([{'$group': {'_id': '$others.amenity', 'count': {'$sum' : 1}}}, {'$sort': {'count' : -1}}])\n",
    "\n",
    "-> here is the result: (the first result with null is the total number of nodes without tag amenities)\n",
    "![Count of Amenities](./amenities.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional ideas\n",
    "As the xml format is flexible we could think of adding many more information, linked to the different amenities listed in the map. For restaurant, for example, users could add reviews of the food, pictures, comments (the way google is doing it). Of courses, this would imply a good moderator system, not to publish incorrect data and make sure that the comments are fair for example, that the picture are the one showing the restaurant...\n",
    "\n",
    "Another idea could be to implement a layer, on top of the map, that shows with different colors the level of completeness of the map, helping willing users to work on the porrly documented areas first. For the area I analysed with coordinate between -3,99272 and -3,72681 (middle point being -3,834) and 47,7684 and 47,94464 (middle point 47,85), I could divide the area in 4 squares and count per squares the number of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zone 1 (lower left area)\n",
    "> db.francefinisteresud.find({$and: [{'pos.0': {$lt: 47.85}, {'pos.1': {$lte: -3.834}}]}).count()\n",
    "-> 38449\n",
    "\n",
    "Zone 2 (upper right area)\n",
    "> db.francefinisteresud.find({$and: [{'pos.0': {$gte: 47.85}, {'pos.1': {$gt: -3.834}}]}).count()\n",
    "-> 33351\n",
    "\n",
    "Zone 3 (lower right area)\n",
    "> db.francefinisteresud.find({$and: [{'pos.0': {$lt: 47.85}, {'pos.1': {$gt: -3.834}}]}).count()\n",
    "-> 22044\n",
    "\n",
    "Zone 4 (lower right area)\n",
    "> db.francefinisteresud.find({$and: [{'pos.0': {$gte: 47.85}, {'pos.1': {$lte: -3.834}}]}).count()\n",
    "-> 122502\n",
    "\n",
    "The total matches the total number of nodes of 216346. Here we can see that even on a small area like this one, the number of nodes per area is huge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The area audited, cleaned and analysed, showed a very good level of data integrity. Very few mistakes were found. One easy improvment on openstreetmap side, would be to automatically tranform the first letter of a street name to an upper case. This project allowed me to learn a new database language (MongoDB) which is completly different from SQL, but allowing to store data in a much more flexible way, although it can become very messy, very quickly, if no standards are followed.\n",
    "\n",
    "## References\n",
    "- All python scripts used to audit, clean and analyse the data come from the case study provided in the previous lesson\n",
    "- the mongoDB queries were built using https://docs.mongodb.com\n",
    "- queries were executed using Mongoboost software"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
